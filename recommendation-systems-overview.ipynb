{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/masatomurakawamm/recommendation-systems-overview?scriptVersionId=150761839\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"6c454fe0","metadata":{"papermill":{"duration":0.004202,"end_time":"2023-11-15T03:39:28.588603","exception":false,"start_time":"2023-11-15T03:39:28.584401","status":"completed"},"tags":[]},"source":["\n","---\n","### 【SVD, MF (Matrix Factorization), IMF (Implicit MF), and iALS (implicit Alternating Least Square) 】  \n","\n","Assuming that you have a feedback matrix $ X \\in \\mathbb R^{m \\times n}$, where $m$ is the number of users (or queries) and $n$ is the number of items.\n","These algorithms learn:\n","  - A user embedding matrix $U \\in \\mathbb R^{m \\times d}$, where row $i$ is the embedding for user $i$.  \n","  - An item embedding matrix $I \\in \\mathbb R^{n \\times d}$, where row $j$ is the embedding for item $j$.  \n","$d$ is the hyperparameter which represents the number of latent factors.\n","\n"," <img width=\"1000\" src=\"https://developers.google.com/machine-learning/recommendation/images/Matrixfactor.svg\">  \n"," \n","(https://developers.google.com/machine-learning/recommendation/collaborative/matrix)\n","\n","Majoir differences between these four algorithms are objective functions to minimize. In addition, whether the feedback is explicit or implicit shold be considered to choose solution.　Explicit feedback is a form of feedback where users directly express their opinions or evaluations, numerical ratings　or assessments for example. Implicit feedback is indirect feedback obtained from user actions and interactions. For example, clicking on specific products, content or search results, and so on.\n","\n","\n","- **SVD**  \n","feedback: explicit  \n","data preprocessing: NaN is replaced to '0' or 'mean value'.  \n","objective function:  \n","\n","$$\n","\\begin{eqnarray}\n","L(U, V) =  \\sum_{i, j} (X_{ij} - \\langle U_{i}, I_{j} \\rangle)^2 \\\\\n","\\end{eqnarray}\n","$$  \n","\n","  note: SVD is effective when the feedback matrix is dense. However, when the feedback matrix is sparse, the prediction values with SVD would be close to '0' (or 'mean value'). \n","\n","\n","- **Matrix Factorization**  \n","feedback: explicit  \n","data preprocessing: NaN is not replaced.  \n","objective function: \n","\n","$$\n","\\begin{eqnarray}\n","L(U,V) &=& \\sum_{(i, j) \\in \\text{obs}} (X_{ij} - \\langle U_{i}, I_{j} \\rangle)^2 +\\lambda\\left(\\sum_i \\|U_i\\|^2 +\\sum_j\\|I_j\\|^2\\right) \\\\ \n","\\end{eqnarray}\n","$$\n","\n","  The second term is for L2 Regularization, and $\\lambda$ is a hyperparameter which controls the importance of regularization.\n","\n","  note: The point of Matrix Factorization is to ignore NaN values. This improves the weakness of SVD.\n","\n","\n","- **IMF**  \n","feedback: implicit   \n","data preprocessing: NaN is not replaced.  \n","objective function: \n","\n","$$\n","\\begin{eqnarray}\n","L(U,V) \n","&=& \\sum_{i, j} c_{ij} (\\bar{r_{ij}} - \\langle U_{i}, I_{j} \\rangle)^2 + \\lambda \\left( \\sum_{i} ||U_i||^2 + \\sum_{j} ||I_j||^2 \\right) \\\\\n","\\end{eqnarray}\n","$$  \n","\n","  IMF considers the number of views, or $r_{ij}$; the number of views of user $i$ for item $j$. The binary variable, $\\bar{r_{ij}}$ is calculated based on $r_{ij}$ as follows, and represents the preference of user $i$ to item $j$.  \n","\n","$$\n","\\begin{eqnarray}\n","\\bar{r_{ij}} = \n","    \\begin{cases}\n","        {1 \\ (r_{ij} > 0)} \\\\\n","        {0 \\ (r_{ij} = 0)} \\\\\n","    \\end{cases}\n","\\end{eqnarray}\n","$$  \n","\n","  Additionally, the confidence of the preference, or $c_{ij}$, is also calculated based on $r_{ij}$. Confidence increases proportionally to $r_{ij}$, and $\\alpha$ is a hyperparameter.  \n","\n","$$\n","\\begin{eqnarray}\n","c_{ij} = 1 + \\alpha r_{ij} \\\\\n","\\end{eqnarray}\n","$$  \n","\n","  When the feedback is implicit, non-observed data points are treated as negative examples, and all the data points (user $\\times$ items) are summed for the evaluation. \n","\n","\n","- **iALS**   \n","feedback: implicit  \n","data preprocessing: NaN is not replaced.  \n","objective function: \n","\n","$$\n","\\begin{eqnarray}\n","L(U,V) &=& \\sum_{i, j} (X_{ij} - \\langle U_{i}, I_{j} \\rangle)^2 +\\lambda\\left(\\sum_i \\|U_i\\|^2 +\\sum_j\\|I_j\\|^2\\right) \\\\\n","&=& \\sum_{(i, j) \\in \\text{obs}} (\\langle U_{i}, I_{j} \\rangle - 1)^2 + w_0 \\sum_{(i, j) \\not \\in \\text{obs}} (\\langle U_i, I_j\\rangle - 0)^2 +  \\lambda\\left(\\sum_i \\|U_i\\|^2 +\\sum_j\\|I_j\\|^2\\right) \\\\\n","&=& \\sum_{(i, j) \\in \\text{obs}} (\\langle U_{i}, I_{j} \\rangle - 1)^2 + w_0 \\sum_{(i, j) \\not \\in \\text{obs}} \\langle U_i, I_j\\rangle^2 +  \\lambda\\left(\\sum_i \\|U_i\\|^2 +\\sum_j\\|I_j\\|^2\\right) \\\\\n","\\end{eqnarray}\n","$$ \n","\n","  note: The second term is a sum over unobserved entries (treated as zeroes), and $w_0$ is a hyperparameter that controls the importance of observed and unobserved entries.\n","\n","\n","---\n","  By using these objective functions, $U$ and $V$ are computed with SGD(Stochastic Gradient Descent) or ALS(Alternating Least Squares).\n","\n","---"]},{"cell_type":"markdown","id":"eba96b5b","metadata":{"papermill":{"duration":0.001925,"end_time":"2023-11-15T03:39:28.59288","exception":false,"start_time":"2023-11-15T03:39:28.590955","status":"completed"},"tags":[]},"source":["### 【Factorization Machines (FM)】\n","\n","The data structure of Factorization Machines is different from that of SVD. IDs of users and items are treated as categorical (one-hot) columns. Features of attribute information(for example, age of users or price of items) and evaluation values (or labels) are also added as column features and answer label, as respectively. As a result, one evaluation sample is treated as one data point, as follows:\n","\n","| sample | user 1 | user 2 | … | item 1 | item 2 | … | feature 1 | feature 2 | … | label |\n","| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n","| $x^{1}$ | 1 | 0 | … | 1 | 0 | … | 0.6 | 1.1 | … | $y^{1}$ |\n","| $x^{2}$ | 1 | 0 | … | 0 | 1 | … | 0.2 | 0.8 | … | $y^{2}$ |\n","| $x^{3}$ | 1 | 0 | … | 0 | 0 | … | 0.7 | 0.1 | … | $y^{3}$ |\n","| $x^{4}$ | 0 | 1 | … | 1 | 0 | … | 0.4 | 0.2 | … | $y^{4}$ |\n","| $x^{5}$ | 0 | 1 | … | 0 | 1 | … | 0.3 | 0.5 | … | $y^{5}$ |\n","| … | … | … | … | … | … | … | … | … | … | … |\n","| $x^{n}$ | 0 | 0 | … | 0 | 0 | … | 1.0 | 0.9 | … | $y^{n}$ |\n","\n","- The most simple FM predicts $\\hat{y}$ as following linear model:\n","\n","$$\n","\\begin{aligned} \n","\\hat{y^{(i)}} = \\left\\langle \\boldsymbol{w},\\boldsymbol{x^{(i)}}\\right\\rangle = w_0 + \\sum_{j=1}^{D}w_{j}x^{(i)}_{j}\\end{aligned}\n","$$\n","\n","$$\n","\\boldsymbol{w}\\in\\mathbb{R}^{D},\n","\\boldsymbol{x}\\in\\mathbb{R}^{n \\times D}\n","$$\n","\n","$\\boldsymbol{w}$ is a parameter vector to learn.\n","\n","- Assuming that you add the interaction term to the linear model. The simple solution is as follows:\n","\n","$$\n","\\begin{aligned} \n","\\hat{y^{(i)}} = \\left\\langle \\boldsymbol{w},\\boldsymbol{x^{(i)}}\\right\\rangle +\\sum_{l,m}w_{l,m}x^{(i)}_{l}x^{(i)}_{m}\\end{aligned} \n","$$\n","\n","However, this model contains $D \\times D$ parameters, and sparsity of feedback makes it hard to learn the whole interaction parameters.\n","\n","- More common (quadratic) FM predicts $\\hat{y}$ as follows:\n","\n","$$\n","\\begin{aligned} \n","\\hat{y^{(i)}} = & w_0 + \\left\\langle \\boldsymbol{w},\\boldsymbol{x^{(i)}}\\right\\rangle +\\sum_{l=1}^{D}\\sum_{m=l+1}^{D}\\left\\langle \\boldsymbol{v}_{l},\\boldsymbol{v}_{m}\\right\\rangle x^{(i)}_{l}x^{(i)}_{m}\\nonumber \\\\ \n","= & w_0 + \\left\\langle \\boldsymbol{w},\\boldsymbol{x^{(i)}}\\right\\rangle +\\sum_{(l, m)\\in C}\\left\\langle \\boldsymbol{v}_{l},\\boldsymbol{v}_{m}\\right\\rangle x^{(i)}_{l}x^{(i)}_{m}\\end{aligned}\n","$$\n","\n","$v_{l}, v_{m} \\in\\mathbb{R}^{k}$ are called latent vectors. $k$ is a hyperparameter of embedding dimention. FM represents the importance of interactions between $x_{l}$ and $x_{m}$ as the inner product of $v_{l}$ and $v_{m}$, $\\left\\langle v_{l}, v_{m}\\right\\rangle$. This helps to learn indirectly the weights of interactions which is not observed. Moreover, computation of interaction parts takes $O(kD)$, not $O(kD^2)$, because of formula transformation.\n","\n","- **Field-Aware Factorization Machines (FFM)**\n","\n","In FM, interactions of features are equally represented as inner products of latent vectors, $v$. FFM added a new index to $v$, and represents the prediction of $y$ as follows:\n","\n","$$\n","\\hat{y^{(i)}} = w_0 + \\sum_j w_j x_j^{(i)} + \\sum_{(l, m)\\in C} \\langle v_{l,F(m)}, v_{m,F(l)} \\rangle x_l x_m\n","$$\n","\n","$F_{l}$ is the field which $x_{l}$ belongs to, and $v_{l,F(m)}$ is the embedding of $x_{l}$ for the field $F_{m}$. FFM has the same number of latend embeddings as the number of fields, and learns all of them. This achieves the representations of interactions with factors unique to each field. However, you must note that there is a risk of overfitting (and there are many countermeasures, of course).\n","\n","FM and FFM are widely applied with many kinds of improvements, such as combination with DNN."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30458,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":14.270328,"end_time":"2023-11-15T03:39:29.523181","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-15T03:39:15.252853","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}